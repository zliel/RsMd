<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Tokenizes a line of markdown text into a vector of `Token` enums."><title>tokenize in markrs::lexer - Rust</title><script>if(window.location.protocol!=="file:")document.head.insertAdjacentHTML("beforeend","SourceSerif4-Regular-6b053e98.ttf.woff2,FiraSans-Italic-81dc35de.woff2,FiraSans-Regular-0fe48ade.woff2,FiraSans-MediumItalic-ccf7e434.woff2,FiraSans-Medium-e1aa3f0a.woff2,SourceCodePro-Regular-8badfe75.ttf.woff2,SourceCodePro-Semibold-aa29a496.ttf.woff2".split(",").map(f=>`<link rel="preload" as="font" type="font/woff2" crossorigin href="../../static.files/${f}">`).join(""))</script><link rel="stylesheet" href="../../static.files/normalize-9960930a.css"><link rel="stylesheet" href="../../static.files/rustdoc-1a91846b.css"><meta name="rustdoc-vars" data-root-path="../../" data-static-root-path="../../static.files/" data-current-crate="markrs" data-themes="" data-resource-suffix="" data-rustdoc-version="1.88.0 (6b00bc388 2025-06-23)" data-channel="1.88.0" data-search-js="search-f7877310.js" data-settings-js="settings-5514c975.js" ><script src="../../static.files/storage-4e99c027.js"></script><script defer src="sidebar-items.js"></script><script defer src="../../static.files/main-7ef8a74a.js"></script><noscript><link rel="stylesheet" href="../../static.files/noscript-893ab5e7.css"></noscript><link rel="alternate icon" type="image/png" href="../../static.files/favicon-32x32-6580c154.png"><link rel="icon" type="image/svg+xml" href="../../static.files/favicon-044be391.svg"></head><body class="rustdoc fn"><!--[if lte IE 11]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="mobile-topbar"><button class="sidebar-menu-toggle" title="show sidebar"></button></nav><nav class="sidebar"><div class="sidebar-crate"><h2><a href="../../markrs/index.html">markrs</a><span class="version">1.3.2</span></h2></div><div class="sidebar-elems"><section id="rustdoc-toc"><h2 class="location"><a href="#">tokenize</a></h2><h3><a href="#">Sections</a></h3><ul class="block top-toc"><li><a href="#arguments" title="Arguments">Arguments</a></li><li><a href="#returns" title="Returns">Returns</a></li><li><a href="#example" title="Example">Example</a></li></ul></section><div id="rustdoc-modnav"><h2><a href="index.html">In markrs::<wbr>lexer</a></h2></div></div></nav><div class="sidebar-resizer"></div><main><div class="width-limiter"><rustdoc-search></rustdoc-search><section id="main-content" class="content"><div class="main-heading"><div class="rustdoc-breadcrumbs"><a href="../index.html">markrs</a>::<wbr><a href="index.html">lexer</a></div><h1>Function <span class="fn">tokenize</span><button id="copy-path" title="Copy item path to clipboard">Copy item path</button></h1><rustdoc-toolbar></rustdoc-toolbar><span class="sub-heading"><a class="src" href="../../src/markrs/lexer.rs.html#26-200">Source</a> </span></div><pre class="rust item-decl"><code>pub fn tokenize(markdown_line: &amp;<a class="primitive" href="https://doc.rust-lang.org/1.88.0/std/primitive.str.html">str</a>) -&gt; <a class="struct" href="https://doc.rust-lang.org/1.88.0/alloc/vec/struct.Vec.html" title="struct alloc::vec::Vec">Vec</a>&lt;<a class="enum" href="../types/enum.Token.html" title="enum markrs::types::Token">Token</a>&gt;</code></pre><details class="toggle top-doc" open><summary class="hideme"><span>Expand description</span></summary><div class="docblock"><p>Tokenizes a line of markdown text into a vector of <code>Token</code> enums.</p>
<h2 id="arguments"><a class="doc-anchor" href="#arguments">ยง</a>Arguments</h2>
<ul>
<li><code>markdown_line</code> - A string slice representing a line of markdown text.</li>
</ul>
<h2 id="returns"><a class="doc-anchor" href="#returns">ยง</a>Returns</h2>
<p>A vector of <code>Token</code> enums representing the tokenized line.</p>
<h2 id="example"><a class="doc-anchor" href="#example">ยง</a>Example</h2>
<div class="example-wrap"><pre class="rust rust-example-rendered"><code><span class="kw">use </span>lexer::tokenize;
<span class="kw">use </span>types::Token;
<span class="kw">let </span>tokens = tokenize(<span class="string">"This is *italic* and **bold** text."</span>);
<span class="macro">assert_eq!</span>(tokens.len(), <span class="number">9</span>);
<span class="macro">assert_eq!</span>(tokens[<span class="number">4</span>], Token::EmphasisRun { delimiter: <span class="string">'*'</span>, length: <span class="number">1 </span>});</code></pre></div>
</div></details></section></div></main></body></html>